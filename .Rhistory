suppressWarnings(dir.create("sample"))
# Create a sample of 1% of the lines
set.seed(999)
writeLines(sample_lines("en_US.blogs.txt", noBlog*.01, noBlog), "sample/blogs.txt")
writeLines(sample_lines("en_US.twitter.txt", noTwit*.01, noTwit), "sample/twitter.txt")
writeLines(sample_lines("en_US.news.txt", noNews*.01, noNews), "sample/news.txt")
# Create a corpus
docs <- VCorpus(DirSource("./sample"))
docs
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, content_transformer(tolower))
dtm <- DocumentTermMatrix(docs)
inspect(dtm)
as.matrix(dtm)
head(as.matrix(dtm))
head(as.data.frame(as.matrix(dtm)))
df <- head(as.data.frame(as.matrix(dtm)))
View(df)
colSums(as.data.frame(rowSums(df)))
ncol(df)
colSums((df))[4]
getCoverage <- function(percentage, dtm) {
#Convert TDM
df <- as.data.frame(as.matrix(dtm))
currentFrequency <- 0
totalFrequency <- colSums(as.data.frame(rowSums(df)))
# Calculate required frequency to reach
requiredFrequency <- percentage * totalFrequency
# Work through dtm row by row
for (i in 1:ncol(df)) {
frequencyOfCol = colSums(df)[i]
currentFrequency <- currentFrequency + frequencyOfRow
# If we have reached the required frequency return the number of rows covered
if (currentFrequency > requiredFrequency) {
return (i)
}
}
}
getCoverage(0.5, dtm)
getCoverage <- function(percentage, dtm) {
#Convert TDM
df <- as.data.frame(as.matrix(dtm))
currentFrequency <- 0
totalFrequency <- colSums(as.data.frame(rowSums(df)))
# Calculate required frequency to reach
requiredFrequency <- percentage * totalFrequency
# Work through dtm row by row
for (i in 1:ncol(df)) {
frequencyOfCol = colSums(df)[i]
currentFrequency <- currentFrequency + frequencyOfCol
# If we have reached the required frequency return the number of rows covered
if (currentFrequency > requiredFrequency) {
return (i)
}
}
}
getCoverage(0.5, dtm)
getCoverage <- function(percentage, dtm) {
#Convert TDM
df <- as.data.frame(as.matrix(dtm))
currentFrequency <- 0
totalFrequency <- colSums(as.data.frame(rowSums(df)))
# Calculate required frequency to reach
requiredFrequency <- percentage * totalFrequency
# Work through dtm row by row
for (i in 1:ncol(df)) {
print(i)
frequencyOfCol = colSums(df)[i]
currentFrequency <- currentFrequency + frequencyOfCol
# If we have reached the required frequency return the number of rows covered
if (currentFrequency > requiredFrequency) {
return (i)
}
}
}
getCoverage <- function(percentage, dtm) {
#Convert TDM
df <- as.data.frame(as.matrix(dtm))
currentFrequency <- 0
totalFrequency <- colSums(as.data.frame(rowSums(df)))
# Calculate required frequency to reach
requiredFrequency <- percentage * totalFrequency
# Work through dtm row by row
for (i in 1:ncol(df)) {
print(i)
frequencyOfCol = colSums(df)[i]
currentFrequency <- currentFrequency + frequencyOfCol
# If we have reached the required frequency return the number of rows covered
if (currentFrequency > requiredFrequency) {
return (i)
}
}
}
getCoverage(0.5, dtm)
sum(df[i,])
sum(df[3,])
sum(df[,3])
sum(df[,18])
getCoverage <- function(percentage, dtm) {
#Convert TDM
df <- as.data.frame(as.matrix(dtm))
currentFrequency <- 0
totalFrequency <- colSums(as.data.frame(rowSums(df)))
# Calculate required frequency to reach
requiredFrequency <- percentage * totalFrequency
# Work through dtm row by row
for (i in 1:ncol(df)) {
print(i)
frequencyOfCol = sum(df[,i])
currentFrequency <- currentFrequency + frequencyOfCol
# If we have reached the required frequency return the number of rows covered
if (currentFrequency > requiredFrequency) {
return (i)
}
}
}
getCoverage(0.5, dtm)
getCoverage <- function(percentage, dtm) {
#Convert TDM
df <- as.data.frame(as.matrix(dtm))
currentFrequency <- 0
totalFrequency <- colSums(as.data.frame(rowSums(df)))
# Calculate required frequency to reach
requiredFrequency <- percentage * totalFrequency
# Work through dtm row by row
for (i in 1:ncol(df)) {
frequencyOfCol = sum(df[,i])
currentFrequency <- currentFrequency + frequencyOfCol
# If we have reached the required frequency return the number of rows covered
if (currentFrequency > requiredFrequency) {
return (i)
}
}
}
getCoverage(0.5, dtm)
getCoverage(0.9, dtm)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = '~/desktop/final/en_US')
library(tm)
library(wordcloud)
library(ngram)
library(hunspell)
library(LaF)
library(NLP)
library(Rgraphviz)
library(dplyr)
library(ggplot2)
# Find the number of lines
noBlog <- determine_nlines('en_US.blogs.txt')
noTwit <- determine_nlines('en_US.twitter.txt')
noNews <- determine_nlines('en_US.news.txt')
# Create a sample directory
suppressWarnings(dir.create("sample"))
# Create a sample of 1% of the lines
set.seed(999)
writeLines(sample_lines("en_US.blogs.txt", noBlog*.01, noBlog), "sample/blogs.txt")
writeLines(sample_lines("en_US.twitter.txt", noTwit*.01, noTwit), "sample/twitter.txt")
writeLines(sample_lines("en_US.news.txt", noNews*.01, noNews), "sample/news.txt")
# Create a corpus
docs <- VCorpus(DirSource("./sample"))
docs
lapply(docs, summary)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, content_transformer(tolower))
dtm <- DocumentTermMatrix(docs)
inspect(dtm)
hunspell(dtm)
hunspell(docs)
hunspell(docs[[1]])
hunspell(docs[1])
docs{1}
docs[1]
docs[[1]]
inspect(docs[[1]])
hunspell(inspect(docs[[1]]))
lapply(lapply(ovid[1:2], as.character), hunspell)
lapply(lapply(docs[1:3], as.character), hunspell)
getCoverage <- function(percentage, dtm) {
#Convert TDM
df <- as.data.frame(as.matrix(dtm))
currentFrequency <- 0
totalFrequency <- colSums(as.data.frame(rowSums(df)))
# Calculate required frequency to reach
requiredFrequency <- percentage * totalFrequency
# Work through dtm row by row
for (i in 1:ncol(df)) {
frequencyOfCol = sum(df[,i])
currentFrequency <- currentFrequency + frequencyOfCol
# If we have reached the required frequency return the number of rows covered
if (currentFrequency > requiredFrequency) {
return (i)
}
}
}
getCoverage(0.5, dtm)
df <- as.data.frame(as.matrix(dtm))
View(df)
hunspell("true")
hunspell(["true"])
hunspell_stem(c("true")
)
df[,1]
colNames(df)
colnames(df)
df <- colnames(as.data.frame(as.matrix(dtm)))
trueVector <- hunspell_stem(df)
typeof(df)
df <- colnames(as.matrix(dtm))
df <- df$hr
df <- as.matrix(dtm)
View(df)
head(colnames(df))
head(colnames(as.matrix(dtm)))
df <- colnames(as.matrix(dtm)))
df <- colnames(as.matrix(dtm))
df[2]
as.list(df)
df <- as.list(colnames(as.matrix(dtm)))
hunspell_stem(df)
hunspell_stem(df)
words <- c("love", "loving", "lovingly", "loved", "lover", "lovely")
typeof(words)
typeof(df)
df <- colnames(as.matrix(dtm))
hunspell_stem(df)
hunspell_stem(c("word"))
hunspell(df)
trueVector <- hunspell(df)
trueVector[1]
is.na(trueVector[1])
sum(identical(trueVector, character(0)))
sum(identical(trueVector, "begley")
)
table(identical(trueVector, "begley"))
lapply(trueVector, print)
countForeignWords <- function(dtm) {
#Convert TDM to vector of words
df <- colnames(as.matrix(dtm))
#Create vector of true / false for in English language
trueVector <- hunspell(df)
count <- 0
lapply(trueVector, function(x) {
if (identical(x, character(0))) {
count <- count + 1
}
})
return(count)
}
countForeignWords(dtm)
countForeignWords <- function(dtm) {
#Convert TDM to vector of words
df <- colnames(as.matrix(dtm))
#Create vector of true / false for in English language
trueVector <- hunspell(df)
count <- 0
lapply(trueVector, function(x) {
print(x)
if (identical(x, character(0))) {
count <- count + 1
}
})
return(count)
}
countForeignWords(dtm)
countForeignWords <- function(dtm) {
#Convert TDM to vector of words
df <- colnames(as.matrix(dtm))
#Create vector of true / false for in English language
trueVector <- hunspell(df)
count <- 0
lapply(trueVector, function(x) {
if (!identical(x, character(0))) {
print(x)
count <- count + 1
}
})
return(count)
}
countForeignWords(dtm)
countForeignWords <- function(dtm) {
#Convert TDM to vector of words
df <- colnames(as.matrix(dtm))
#Create vector of true / false for in English language
trueVector <- hunspell(df)
count <- 0
lapply(trueVector, function(x) {
if (!identical(x, character(0))) {
count <- count + 1
}
})
return(count)
}
countForeignWords(dtm)
countForeignWords <- function(dtm) {
#Convert TDM to vector of words
df <- colnames(as.matrix(dtm))
#Create vector of true / false for in English language
trueVector <- hunspell(df)
count <- 0
foreignWords <- lapply(trueVector, function(x) {
if (!identical(x, character(0))) {
return(x)
}
})
return(length(foreignWords))
}
countForeignWords(dtm)
countForeignWords <- function(dtm) {
#Convert TDM to vector of words
df <- colnames(as.matrix(dtm))
#Create vector of true / false for in English language
trueVector <- hunspell(df)
count <- 0
foreignWords <- lapply(trueVector, function(x) {
if (!identical(x, character(0))) {
return(x)
} else {
return(NULL)
}
})
return(length(foreignWords))
}
countForeignWords(dtm)
length(trueVector[1])
length(trueVector[0])
length(trueVector[3])
length(trueVector[4])
head(trueVector)
head(trueVector[[1]])
length(trueVector[[1]])
countForeignWords <- function(dtm) {
#Convert TDM to vector of words
df <- colnames(as.matrix(dtm))
#Create vector of true / false for in English language
trueVector <- hunspell(df)
count <- 0
for (i in 1:length(trueVector)) {
if (length(trueVector[[i]]) != 0) {
count <- count + 1
}
}
return(count)
}
countForeignWords(dtm)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = '~/desktop/final/en_US')
library(tm)
library(wordcloud)
library(ngram)
library(hunspell)
library(LaF)
library(NLP)
library(Rgraphviz)
library(dplyr)
library(ggplot2)
# Find the number of lines
noBlog <- determine_nlines('en_US.blogs.txt')
noTwit <- determine_nlines('en_US.twitter.txt')
noNews <- determine_nlines('en_US.news.txt')
# Create a sample directory
suppressWarnings(dir.create("sample"))
# Create a sample of 1% of the lines
set.seed(999)
writeLines(sample_lines("en_US.blogs.txt", noBlog*.01, noBlog), "sample/blogs.txt")
writeLines(sample_lines("en_US.twitter.txt", noTwit*.01, noTwit), "sample/twitter.txt")
writeLines(sample_lines("en_US.news.txt", noNews*.01, noNews), "sample/news.txt")
# Create a corpus
docs <- VCorpus(DirSource("./sample"))
docs
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, content_transformer(tolower))
# Create new documents with word stemming performed
stemDocs = tm_map(docs, stemDocument)
# Create new documents with word stemming performed
stemDocs = tm_map(docs, stemDocument)
# Create stemmed dtm
stemDtm <- DocumentTermMatrix(stemDocs)
# Calculate required words for 50% coverage
getCoverage(0.5, stemDtm)
getCoverage <- function(percentage, dtm) {
#Convert TDM
df <- as.data.frame(as.matrix(dtm))
currentFrequency <- 0
totalFrequency <- colSums(as.data.frame(rowSums(df)))
# Calculate required frequency to reach
requiredFrequency <- percentage * totalFrequency
# Work through dtm column by column
for (i in 1:ncol(df)) {
frequencyOfCol = sum(df[,i])
currentFrequency <- currentFrequency + frequencyOfCol
# If we have reached the required frequency return the number of rows covered
if (currentFrequency > requiredFrequency) {
return (i)
}
}
}
# Create new documents with word stemming performed
stemDocs = tm_map(docs, stemDocument)
# Create stemmed dtm
stemDtm <- DocumentTermMatrix(stemDocs)
# Calculate required words for 50% coverage
getCoverage(0.5, stemDtm)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = '~/desktop/final/en_US')
library(tm)
library(wordcloud)
library(ngram)
library(hunspell)
library(LaF)
library(NLP)
library(Rgraphviz)
library(dplyr)
library(ggplot2)
set.seed(201)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = '~/desktop/final/en_US')
library(tm)
library(wordcloud)
library(ngram)
library(hunspell)
library(LaF)
library(NLP)
library(Rgraphviz)
library(dplyr)
library(ggplot2)
# Find the number of lines
noBlog <- determine_nlines('en_US.blogs.txt')
noTwit <- determine_nlines('en_US.twitter.txt')
noNews <- determine_nlines('en_US.news.txt')
# Create a sample directory
suppressWarnings(dir.create("sample"))
# Create a sample of 1% of the lines
set.seed(999)
writeLines(sample_lines("en_US.blogs.txt", noBlog*.01, noBlog), "sample/blogs.txt")
writeLines(sample_lines("en_US.twitter.txt", noTwit*.01, noTwit), "sample/twitter.txt")
writeLines(sample_lines("en_US.news.txt", noNews*.01, noNews), "sample/news.txt")
# Create a corpus
docs <- VCorpus(DirSource("./sample"))
docs
lapply(docs, summary)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, content_transformer(tolower))
dtm <- DocumentTermMatrix(docs)
inspect(dtm)
sums <- sort(colSums(as.matrix(dtm)), decreasing = TRUE)
freq <- data.frame(word=names(sums), freq=sums)
freq20 <- head(freq, 20)
ggplot(aes(word, freq), data = freq20) + geom_bar(stat="identity", fill="darkred")
freq60 <- head(freq, 60)
wordcloud(freq60$word, freq60$freq)
# Create a tokeniser to find groups of two words
tokeniser <- function(x) unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
# Create a dtm with the tokeniser
dtmNgram2 <- DocumentTermMatrix(docs, control=list(tokenize = tokeniser))
inspect(dtmNgram2)
# Create a tokeniser to find groups of three words
tokeniser <- function(x) unlist(lapply(ngrams(words(x), 3), paste, collapse = " "), use.names = FALSE)
# Create a dtm with the tokeniser
dtmNgram3 <- DocumentTermMatrix(docs, control = list(tokenize = tokeniser))
inspect(dtmNgram3)
sums <- sort(colSums(as.matrix(dtmNgram2)), decreasing = TRUE)
freq2 <- data.frame(word=names(sums), freq=sums)
freq20 <- head(freq2, 20)
ggplot(aes(word, freq), data = freq20) + geom_bar(stat="identity", fill="darkred") + theme(axis.text.x = element_text(angle = 90, hjust = 1))
sums <- sort(colSums(as.matrix(dtmNgram3)), decreasing = TRUE)
freq3 <- data.frame(word=names(sums), freq=sums)
freq20 <- head(freq3, 20)
ggplot(aes(word, freq), data = freq20) + geom_bar(stat="identity", fill="darkred") + theme(axis.text.x = element_text(angle = 90, hjust = 1))
freq60 <- head(freq2, 60)
wordcloud(freq60$word, freq60$freq)
freq10 <- head(freq3, 10)
wordcloud(freq10$word, freq10$freq)
getCoverage <- function(percentage, dtm) {
#Convert TDM
df <- as.data.frame(as.matrix(dtm))
currentFrequency <- 0
totalFrequency <- colSums(as.data.frame(rowSums(df)))
# Calculate required frequency to reach
requiredFrequency <- percentage * totalFrequency
# Work through dtm column by column
for (i in 1:ncol(df)) {
frequencyOfCol = sum(df[,i])
currentFrequency <- currentFrequency + frequencyOfCol
# If we have reached the required frequency return the number of rows covered
if (currentFrequency > requiredFrequency) {
return (i)
}
}
}
getCoverage(0.5, dtm)
getCoverage(0.9, dtm)
countForeignWords <- function(dtm) {
#Convert TDM to vector of words
df <- colnames(as.matrix(dtm))
#Create vector of true / false for in English language
trueVector <- hunspell(df)
count <- 0
for (i in 1:length(trueVector)) {
if (length(trueVector[[i]]) != 0) {
count <- count + 1
}
}
return(count)
}
countForeignWords(dtm)
# Create new documents with word stemming performed
stemDocs = tm_map(docs, stemDocument)
# Create stemmed dtm
stemDtm <- DocumentTermMatrix(stemDocs)
# Calculate required words for 50% coverage
getCoverage(0.5, stemDtm)
shiny::runApp('Desktop/final/Coursera_Data_Science_Capstone')
